### Метрики оценки качества моделей машинного обучения

При решении задач классификации важно оценивать, насколько хорошо модель предсказывает классы. Для этого используются следующие метрики:

---

## 1. Основные понятия: TP, TN, FP, FN

Перед тем как разбирать метрики, определим ключевые термины:

- **TP (True Positive, Истинно положительные)** – объекты, которые модель **правильно классифицировала** как **положительный класс**.
- **TN (True Negative, Истинно отрицательные)** – объекты, которые модель **правильно классифицировала** как **отрицательный класс**.
- **FP (False Positive, Ложно положительные)** – объекты, которые модель **ошибочно классифицировала** как **положительный класс**, но они на самом деле **отрицательные**.
- **FN (False Negative, Ложно отрицательные)** – объекты, которые модель **ошибочно классифицировала** как **отрицательный класс**, но они на самом деле **положительные**.

Пример матрицы ошибок:

|                | Предсказано: Положительный класс | Предсказано: Отрицательный класс |
|--------------|--------------------------------|--------------------------------|
| **Фактически Положительный** | TP (Истинно положительные) | FN (Ложно отрицательные) |
| **Фактически Отрицательный** | FP (Ложно положительные) | TN (Истинно отрицательные) |

---

## 2. Accuracy (Точность классификации)

**Формула:**  
$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

**Описание:**  
Accuracy показывает долю **правильных** предсказаний среди всех примеров. Однако, если классы несбалансированы (например, один класс встречается гораздо чаще), **accuracy может вводить в заблуждение**.

**Пример:**  
Если модель правильно предсказала 80% примеров, то `Accuracy = 0.80` (или 80%).

---

## 3. Precision (Точность)

**Формула:**  
$$
Precision = \frac{TP}{TP + FP}
$$

**Описание:**  
Из всех объектов, которые модель предсказала как **положительный класс**, какая доля действительно положительная.

**Использование:**  
Precision важен в задачах, где ошибки False Positive критичны (например, в детекции спама: лучше пропустить одно письмо со спамом, чем ошибочно пометить важное письмо как спам).

---

## 4. Recall (Полнота)

**Формула:**  
$$
Recall = \frac{TP}{TP + FN}
$$

**Описание:**  
Из всех объектов **положительного класса**, сколько модель **смогла правильно найти**.

**Использование:**  
Recall важен в задачах, где критичны False Negative (например, в диагностике болезней: лучше допустить ложное срабатывание, чем пропустить серьёзный диагноз).

---

## 5. F1-score (Сбалансированная оценка)

**Формула:**  
$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

**Описание:**  
F1-метрика — это **среднее гармоническое Precision и Recall**, она балансирует их влияние.

**Использование:**  
Используется, когда нужно учитывать и Precision, и Recall одновременно.  
Чем выше **F1-score**, тем лучше модель справляется с задачей.

---

## 6. Пример интерпретации значений:

```
Accuracy: 0.8000   → 80% предсказаний верные.
Precision: 0.8265  → 82.65% объектов, помеченных как положительные, действительно таковыми являются.
Recall: 0.7367     → 73.67% всех положительных объектов было найдено.
F1 Score: 0.7790   → Компромисс между Precision и Recall.
```

**Вывод:**  
- Если Precision высокий, но Recall низкий → модель избегает ошибок False Positive, но может пропускать положительные примеры.  
- Если Recall высокий, но Precision низкий → модель захватывает больше положительных случаев, но среди них много ложных срабатываний.  
- F1 помогает найти баланс между ними.

Если метрики низкие — модель нужно дорабатывать, подбирая **лучшие признаки, алгоритм или гиперпараметры**.